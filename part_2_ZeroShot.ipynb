{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "101dcda8",
   "metadata": {},
   "source": [
    "# Zero-Shot Classification with LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643be79c",
   "metadata": {},
   "source": [
    "\n",
    "Here we are going to use an LLM to classify the patent abstracts. We are using Mistral`s LLM mistral-small-latest which is available through their api as part of the free tier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884a6d7b",
   "metadata": {},
   "source": [
    "The prompts passed to the LLM look something like this:\n",
    "\n",
    "system message:\n",
    "\n",
    "You are an expert patent-classification assistant.\n",
    "Choose exactly one of the following IPC section letters for every abstract you receive\n",
    "and return only that single letter—nothing else.\n",
    "\n",
    "A: Human necessities\n",
    "B: Performing operations; transporting\n",
    "C: Chemistry; metallurgy\n",
    "D: Textiles; paper\n",
    "E: Fixed constructions\n",
    "F: Mechanical engineering; lighting; heating; weapons; blasting\n",
    "G: Physics\n",
    "H: Electricity\n",
    "Y: Emerging cross-sectional technologies\n",
    "\n",
    "user message: Patent abstract: A portable device includes a rechargeable electro-chemical cell coupled to a power-management circuit that wirelessly transmits energy to external loads via an inductive coil. The circuit actively regulates output current to optimize efficiency while protecting the cell from over-discharge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8378f27d",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7d2f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/Users/hannes/Documents/NLP_final/patent_corpus.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df688c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset df for only text and label\n",
    "df = df[['abstract', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "152b8739",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN label counts\n",
      " label\n",
      "a    4\n",
      "b    4\n",
      "f    4\n",
      "h    4\n",
      "y    4\n",
      "c    3\n",
      "d    3\n",
      "e    3\n",
      "g    3\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yl/blfh275s3xq56mz3wkmfnrzh0000gn/T/ipykernel_42481/1767580291.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(sample_k)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "SEED = 42\n",
    "\n",
    "# Always add a stable row_id column *once*\n",
    "df[\"row_id\"] = np.arange(len(df))\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 1)  Build the balanced 32-shot TRAIN set\n",
    "# ------------------------------------------------------------\n",
    "labels        = sorted(df[\"label\"].unique())\n",
    "n_labels      = len(labels)          # 9\n",
    "base          = 32 // n_labels       # → 3\n",
    "remainder     = 32 - base * n_labels # → 5 extra shots\n",
    "\n",
    "np.random.seed(SEED)\n",
    "extra_labels  = np.random.choice(labels, size=remainder, replace=False)\n",
    "\n",
    "def sample_k(grp):\n",
    "    k = base + (1 if grp.name in extra_labels else 0)\n",
    "    return grp.sample(k, random_state=SEED)\n",
    "\n",
    "train_df = (\n",
    "    df.groupby(\"label\", group_keys=False)\n",
    "      .apply(sample_k)\n",
    "      .reset_index(drop=True)\n",
    ")\n",
    "\n",
    "print(\"TRAIN label counts\\n\", train_df[\"label\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c08df453",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/patentcls/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, Features, Value, ClassLabel\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 2)  Everything else → TEMP pool\n",
    "# ------------------------------------------------------------\n",
    "temp_df = df.loc[~df[\"row_id\"].isin(train_df[\"row_id\"])].reset_index(drop=True)\n",
    "\n",
    "# When you make eval_df and test_holdout_df\n",
    "eval_df, test_holdout_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.90,\n",
    "    stratify=temp_df[\"label\"],\n",
    "    random_state=SEED\n",
    ")\n",
    "eval_df         = eval_df.reset_index(drop=True)        # <—\n",
    "test_holdout_df = test_holdout_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# 4)\n",
    "# ------------------------------------------------------------\n",
    "# treat test_holdout_df as “unlabeled” by removing the ground-truth labels\n",
    "pseudo_pool_df = (\n",
    "    test_holdout_df\n",
    "      .drop(columns=[\"label\"])     # <-- removes the gold labels\n",
    "      .reset_index(drop=True)      # keep a clean index\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a573c280",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF splits ready: 32 6703 60333 60333\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------\n",
    "# 5)  Convert each to HF Dataset  (future-proof & RAM-safe)\n",
    "# ------------------------------------------------------------\n",
    "feature_spec = Features({\n",
    "    \"text\" : Value(\"string\"),\n",
    "    \"label\": ClassLabel(names=labels),\n",
    "    \"row_id\": Value(\"int64\")\n",
    "})\n",
    "\n",
    "# Create a feature spec for the pseudo_pool_ds which does not have a 'label' column\n",
    "pseudo_pool_feature_spec = Features({\n",
    "    \"text\" : Value(\"string\"),\n",
    "    \"row_id\": Value(\"int64\")\n",
    "})\n",
    "\n",
    "train_ds        = Dataset.from_pandas(train_df.rename(columns={\"abstract\":\"text\"}),        features=feature_spec)\n",
    "eval_ds         = Dataset.from_pandas(eval_df.rename(columns={\"abstract\":\"text\"}),         features=feature_spec)\n",
    "test_holdout_ds = Dataset.from_pandas(test_holdout_df.rename(columns={\"abstract\":\"text\"}), features=feature_spec)\n",
    "# Use the pseudo_pool_feature_spec for the pseudo_pool_df\n",
    "pseudo_pool_ds  = Dataset.from_pandas(pseudo_pool_df.rename(columns={\"abstract\":\"text\"}),  features=pseudo_pool_feature_spec)\n",
    "\n",
    "print(\"HF splits ready:\",\n",
    "      len(train_ds), len(eval_ds), len(test_holdout_ds), len(pseudo_pool_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d30a07",
   "metadata": {},
   "source": [
    "## Zero-Shot Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "daf9b9c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -qU litellm tqdm\n",
    "import os\n",
    "\n",
    "# ── 1.  API key & model -------------------------------------------------\n",
    "os.environ[\"MISTRAL_API_KEY\"] = \"1jwUcSzw7IwGdusNjHmnmKfMuWpf4qg3\"        #  ← paste your key here\n",
    "MODEL = \"mistral/mistral-small-latest\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a33f5bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LLM zero-shot: 100%|██████████| 6703/6703 [2:24:04<00:00,  1.29s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNK rate: 0.00%\n",
      "Macro-F1: 0.42926781437717637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           a       0.71      0.68      0.69       967\n",
      "           b       0.64      0.11      0.19       897\n",
      "           c       0.60      0.71      0.65       561\n",
      "           d       0.27      0.55      0.37        56\n",
      "           e       0.44      0.42      0.43       191\n",
      "           f       0.25      0.80      0.38       475\n",
      "           g       0.58      0.41      0.48      1438\n",
      "           h       0.55      0.82      0.66      1427\n",
      "           y       0.12      0.00      0.00       691\n",
      "\n",
      "    accuracy                           0.51      6703\n",
      "   macro avg       0.46      0.50      0.43      6703\n",
      "weighted avg       0.53      0.51      0.47      6703\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ================================================================\n",
    "# Zero-shot IPC classification on eval_ds with Mistral (JSON mode)\n",
    "# ================================================================\n",
    "\n",
    "import os, json, time\n",
    "from tqdm.auto import tqdm\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from litellm import completion\n",
    "                # provider prefix if needed\n",
    "\n",
    "# ── 1. label mapping ----------------------------------------------------\n",
    "label_names = train_ds.features[\"label\"].names   # [\"a\",\"b\",...,\"y\"]\n",
    "name2id     = {n.upper(): i for i, n in enumerate(label_names)}\n",
    "\n",
    "IPC_DESCR = {\n",
    "    \"A\": \"Human necessities\",\n",
    "    \"B\": \"Operations / transporting\",\n",
    "    \"C\": \"Chemistry / metallurgy\",\n",
    "    \"D\": \"Textiles / paper\",\n",
    "    \"E\": \"Fixed constructions\",\n",
    "    \"F\": \"Mechanical engineering; lighting; heating; weapons; blasting\",\n",
    "    \"G\": \"Physics\",\n",
    "    \"H\": \"Electricity\",\n",
    "    \"Y\": \"Emerging cross-section technologies\"\n",
    "}\n",
    "\n",
    "SYSTEM_MSG = (\n",
    "    \"You are an expert patent classifier.\\n\"\n",
    "    \"For each user message return **JSON** *exactly* of the form:\\n\"\n",
    "    '{\"section\": \"<LETTER>\"}\\n'\n",
    "    \"where <LETTER> is one of A-H or Y.\\n\"\n",
    "    \"Descriptions:\\n\" +\n",
    "    \"\\n\".join(f\"{k}: {v}\" for k, v in IPC_DESCR.items())\n",
    ")\n",
    "\n",
    "# ── 2. helper -----------------------------------------------------------\n",
    "def predict_label(abstract, temp=0.3):\n",
    "    prompt = f\"Patent abstract:\\n{abstract[:800]}\\n\\nReturn JSON now.\"\n",
    "    resp = completion(\n",
    "        model           = MODEL,\n",
    "        api_key         = os.getenv(\"MISTRAL_API_KEY\"),\n",
    "        messages        = [\n",
    "            {\"role\": \"system\", \"content\": SYSTEM_MSG},\n",
    "            {\"role\": \"user\",   \"content\": prompt}\n",
    "        ],\n",
    "        response_format = {\"type\": \"json_object\"},   # ← JSON mode\n",
    "        temperature     = temp,\n",
    "        max_tokens      = 20,\n",
    "    )\n",
    "    try:\n",
    "        section = json.loads(resp.choices[0].message.content)[\"section\"].upper()\n",
    "        return section if section in IPC_DESCR else \"UNK\"\n",
    "    except Exception:\n",
    "        return \"UNK\"\n",
    "\n",
    "# ── 3. classify ---------------------------------------------------------\n",
    "texts = eval_ds[\"text\"]\n",
    "true  = np.array(eval_ds[\"label\"])\n",
    "pred  = []\n",
    "\n",
    "for t in tqdm(texts, desc=\"LLM zero-shot\"):\n",
    "    section = predict_label(t)\n",
    "    pred.append(name2id.get(section, -1))\n",
    "    time.sleep(1.0)          # stay under ~60 req/min on free tier\n",
    "\n",
    "pred = np.array(pred)\n",
    "valid = pred != -1\n",
    "\n",
    "unk_rate = 1 - valid.mean()\n",
    "print(f\"UNK rate: {unk_rate:.2%}\")\n",
    "\n",
    "macro_f1 = f1_score(true[valid], pred[valid], average=\"macro\") if valid.any() else 0.0\n",
    "print(\"Macro-F1:\", macro_f1)\n",
    "print(classification_report(true[valid], pred[valid], target_names=label_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5d4dc3",
   "metadata": {},
   "source": [
    "Overall the Zero-Shot classification with the LLM works a little better than our best model trained with the 32 labled examples plus the 20 LLM genrated abstracts per label( 43 f1-score vs 36 f1- score). The performance of the LLM is also similar across classes, it is unable to detetect abstract of the label y. The category y is called \"General tagging of new or cross-sectional technology\". This is an inherently ambigous category which will be hard to detect in all cases. Same as in our best SetFit model the performance is quite good for classes a, c, g and h."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "patentcls",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
